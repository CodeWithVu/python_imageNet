{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Setup Environment and Download Data\n",
                "## B∆Ø·ªöC 1: Clone code v√† c√†i ƒë·∫∑t package"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Clone repository n·∫øu ch∆∞a c√≥\n",
                "if not os.path.exists('python_imageNet'):\n",
                "    !git clone -b vu https://github.com/CodeWithVu/python_imageNet.git\n",
                "    print(\"‚úÖ Cloned repository\")\n",
                "else:\n",
                "    print(\"‚úÖ Repository already exists\")\n",
                "\n",
                "# Chuy·ªÉn v√†o th∆∞ m·ª•c project\n",
                "%cd python_imageNet\n",
                "\n",
                "# C√†i ƒë·∫∑t package editable\n",
                "!pip install -q -e .\n",
                "print(\"‚úÖ Package installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## B∆Ø·ªöC 2: Upload v√† c·∫•u h√¨nh Kaggle API\n",
                "### H∆∞·ªõng d·∫´n l·∫•y kaggle.json:\n",
                "1. V√†o https://www.kaggle.com\n",
                "2. Settings ‚Üí API ‚Üí \"Create New API Token\"\n",
                "3. T·∫£i file `kaggle.json` v·ªÅ m√°y\n",
                "4. Ch·∫°y cell d∆∞·ªõi ƒë√¢y v√† upload file kaggle.json khi ƒë∆∞·ª£c y√™u c·∫ßu"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Upload kaggle.json t·ª´ m√°y local (qua VS Code)\n",
                "from google.colab import files\n",
                "\n",
                "print(\"üì§ Vui l√≤ng upload file kaggle.json...\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Setup Kaggle credentials\n",
                "!mkdir -p ~/.kaggle\n",
                "!mv kaggle.json ~/.kaggle/\n",
                "!chmod 600 ~/.kaggle/kaggle.json\n",
                "print(\"‚úÖ Kaggle API configured\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## B∆Ø·ªöC 3: T·∫£i dataset AID t·ª´ Kaggle (ch·∫°y tr√™n cloud)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ki·ªÉm tra n·∫øu data ƒë√£ t·∫£i r·ªìi th√¨ skip\n",
                "if not os.path.exists('./dataset/AID'):\n",
                "    print(\"üì• ƒêang t·∫£i dataset t·ª´ Kaggle...\")\n",
                "    !kaggle datasets download -d cyian/aid-scene-classification-datasets -p ./dataset\n",
                "    \n",
                "    print(\"üì¶ ƒêang gi·∫£i n√©n...\")\n",
                "    !unzip -q ./dataset/aid-scene-classification-datasets.zip -d ./dataset\n",
                "    \n",
                "    print(\"‚úÖ Dataset ready!\")\n",
                "else:\n",
                "    print(\"‚úÖ Dataset already exists\")\n",
                "\n",
                "# Set ƒë∆∞·ªùng d·∫´n data\n",
                "sdir = './dataset/AID'\n",
                "print(f\"üìÅ Data path: {sdir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Main Training Pipeline\n",
                "## Import c√°c module c·∫ßn thi·∫øt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from doan.data_loading import create_dataframe\n",
                "from doan.data_analysis import analyze_dataset\n",
                "from doan.data_trimming import trim_dataframe\n",
                "from doan.generators import create_generators\n",
                "from doan.model_creation import create_model\n",
                "from doan.callbacks_setup import create_callbacks\n",
                "from doan.training import train_model\n",
                "from doan.visualization import plot_training_data, show_image_samples\n",
                "from doan.evaluation import predictor, save_model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load v√† ph√¢n t√≠ch dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset\n",
                "df = create_dataframe(sdir)\n",
                "print(f'üìä Dataset loaded: {df.shape}')\n",
                "\n",
                "# Analyze dataset\n",
                "classes, class_count = analyze_dataset(df)\n",
                "print(f'üè∑Ô∏è  Found {class_count} classes')\n",
                "print(f'Classes: {classes[:5]}... (showing first 5)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## C√¢n b·∫±ng d·ªØ li·ªáu (Data Trimming)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Trim dataset ƒë·ªÉ balance\n",
                "max_samples = 500  # S·ªë samples t·ªëi ƒëa m·ªói class\n",
                "min_samples = 50   # S·ªë samples t·ªëi thi·ªÉu m·ªói class\n",
                "df_trimmed = trim_dataframe(df, max_samples, min_samples, 'labels')\n",
                "print(f'‚úÇÔ∏è  Dataset after trimming: {df_trimmed.shape}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Chia dataset v√† t·∫°o generators"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Split dataset\n",
                "train_df, temp_df = train_test_split(df_trimmed, test_size=0.3, random_state=42, stratify=df_trimmed['labels'])\n",
                "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['labels'])\n",
                "\n",
                "print(f'üîÄ Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}')\n",
                "\n",
                "# Create generators\n",
                "img_size = (224, 224)\n",
                "batch_size = 32\n",
                "train_gen, valid_gen, test_gen, classes, class_count, test_steps = create_generators(train_df, valid_df, test_df, img_size, batch_size)\n",
                "print(f'‚úÖ Generators created')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## T·∫°o m√¥ h√¨nh EfficientNetB3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create EfficientNetB3 model\n",
                "model = create_model(img_size + (3,), class_count)\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hu·∫•n luy·ªán m√¥ h√¨nh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup callbacks\n",
                "epochs = 20\n",
                "ask_epoch = 10\n",
                "callbacks = create_callbacks(model, epochs, ask_epoch)\n",
                "\n",
                "# Train model\n",
                "print(\"üöÄ Starting training...\")\n",
                "history = train_model(model, train_gen, valid_gen, epochs, callbacks)\n",
                "print(\"‚úÖ Training completed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "plot_training_data(history, 0)\n",
                "\n",
                "# Show sample images\n",
                "show_image_samples(train_gen)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ƒê√°nh gi√° v√† l∆∞u m√¥ h√¨nh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict v√† evaluate\n",
                "errors, tests = predictor(model, test_gen, test_steps, classes)\n",
                "\n",
                "# Save model to Colab (ho·∫∑c Google Drive n·∫øu ƒë√£ mount)\n",
                "save_dir = '/content/models'  # Ho·∫∑c '/content/drive/MyDrive/DoAn_Models' n·∫øu mount Drive\n",
                "os.makedirs(save_dir, exist_ok=True)\n",
                "\n",
                "# Thay accuracy th·ª±c t·∫ø sau khi test\n",
                "save_model(model, 'efficientnet_b3_aid', 95.5, save_dir)\n",
                "print(f\"‚úÖ Model saved to {save_dir}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}